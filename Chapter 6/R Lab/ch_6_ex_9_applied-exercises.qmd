---
title: "ISLR Chapter 6 Applied Exercises Solutions"
author: "Ricardo J. Serrano"
date: "October 28, 2023"
format:
  html:
    theme: sandstone
    toc: true
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

```{r setup}
#| include: false
library(knitr)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE,
                      message = FALSE, dpi = 180, 
                      fig.width = 8, fig.height = 5)

suppressMessages(library(tidyverse))
library(olsrr)
library(glmnet)
library(pls)
theme_set(theme_bw())
```

# ISLR Applied Exercises

## Exercise 9

In this exercise, we will predict the number of applications received
using the other variables in the `College` data set.

This dataset was taken from the StatLib library which is maintained at
Carnegie Mellon University. The dataset was used in the ASA Statistical
Graphics Section's 1995 Data Analysis Exposition.

[Source](https://rdrr.io/cran/ISLR/man/College.html)

### EDA

Read the dataset.

```{r 9-read-dataset}
college_df <- read_csv('./Data/College.csv') %>% 
     select(-1) %>% 
     janitor::clean_names() %>% 
     mutate(private = private %>% as.factor)

college_df
```

Skim the dataset.

```{r 9-skim}
skimr::skim(college_df)
```

#### Univariate analysis

Plot histograms for numeric variables

```{r 9-histograms}
library(DataExplorer)

college_df %>% 
     plot_histogram()
```

Bar plots

```{r 9-boxplots}
college_df %>% 
     plot_bar()
```

'apps' boxplot

```{r 9-apps-boxplot}
college_df %>% 
     ggplot(aes(apps)) + 
     geom_boxplot()
```

'apps' log transform boxplot

```{r 9-apps-boxplot}
college_df %>% 
     ggplot(aes(log(apps))) + 
     geom_boxplot()
```

'apps' log transform histogram

```{r 9-apps-histogram}
college_df %>% 
     ggplot(aes(log(apps))) + 
     geom_histogram(fill = 'steelblue')
```

Skewness

```{r 9-skewness}
college_variable_skew_vec <- college_df %>% 
     select_if(is.numeric) %>% 
     moments::skewness() %>% 
     as.data.frame() %>% 
     rownames_to_column() %>% 
     rename(variable = 1, skew = 2) %>% 
     filter(skew > 0.8, variable != 'apps') %>% 
     pull(variable)

college_variable_skew_vec
```

Save `college_variable_skew_vec` as rds file for future use
```{r 9-skewness-vec-save-rds}
college_variable_skew_vec %>% 
     write_rds('./college_variable_skew_vec.rds')
```


#### Bivariate/multivariate analysis

Scatterplots

```{r 9-scatterplots}
college_df %>% 
     plot_scatterplot(by = 'apps')
```

Using tidyr and ggplot2
Source: https://drsimonj.svbtle.com/plot-some-variables-against-many-others
```{r 9-scatterplots-lm}
college_df %>% 
     gather(-apps, -private, key = 'var', value = 'value') %>% 
     ggplot(aes(x = value, y = apps, color = private, shape = private)) + 
     geom_point(alpha = 0.3) + 
     geom_smooth(method = 'lm', se = FALSE) + 
     facet_wrap(~ var, scales = 'free')
```

Correlation plot

```{r 9-correlation}
college_df %>% 
     mutate(private = private %>% as.numeric) %>% 
     plot_correlation()
```

From the correlogram plot, we notice that there is high correlation
between the target ('apps') and the predictors, but also among the
predictors themselves (i.e., presence of multicollinearity).

GGpairs (GGally)
Source: https://www.blopig.com/blog/2019/06/a-brief-introduction-to-ggpairs/
```{r 9-ggpairs}
library(GGally)

college_df %>% 
     select(apps, private, accept, enroll, outstate, ph_d, expend, room_board) %>% 
     ggpairs(., 
             mapping = aes(colour = private), 
             lower = list(continuous = wrap("smooth", 
                                            alpha = 0.3, 
                                            size=0.1))
             )
```

'apps' v. 'accept'

```{r 9-apps-accept-scatter}
college_df %>% 
     ggplot(aes(x = accept, y = apps)) + 
     geom_point(alpha = 0.3) + 
     geom_smooth(method = 'lm', se = FALSE, color = 'red')
```

'apps' v. 'perc_alumni'

```{r 9-apps-perc-alumni-scatter}
college_df %>% 
     ggplot(aes(x = perc_alumni, y = apps)) + 
     geom_point(alpha = 0.3) + 
     geom_smooth(method = 'lm', se = FALSE, color = 'red')
```

'apps' by 'private'

```{r}
college_df %>% 
     ggplot(aes(private, apps, fill = private)) + 
     geom_boxplot() + 
     theme(legend.position = 'none')
```

T-test 'apps' by 'private'

```{r 9-t-test}
t.test(apps ~ private, data = college_df)
```

The results of the unpaired t-test p-value \< 0.05, so we reject the null hypothesis in favor of the alternate (i.e., group means are statistically different).

Before answering the exercise problems, let's create an OLS regression and apply the `ols_step_all_possible()` from the `olsrr` package to get the best subset model from all possible combinations of predictors.

```{r ols-best-subset}
# model_lm <- lm(apps ~ ., data = college_df)
# 
# best_subset <- ols_step_best_subset(model_lm)
# plot(best_subset)
```

### (a) Split the data set into a training set and a test set.

Source: https://rpubs.com/lmorgan95/ISLR_CH6_Solutions

Create a `train` dataset from 70% of the observations.

```{r 9-split_dataset_train}
set.seed(102)

sample_size <- 0.7
train_index <- sample(1:nrow(college_df), round(nrow(college_df) * sample_size))

train <- college_df[train_index, ]
nrow(train) / nrow(college_df)
```

Remaining observations will be allocated in the `test` dataset.

```{r 9-test-dataset}
test <- college_df[-train_index, ]
nrow(test) / nrow(college_df)
```

### (b) Fit a linear model using least squares on the training set, and report the test error obtained.

```{r 9-ols-model}
model_lm <- lm(apps ~ ., data = train)
summary(model_lm)
```

Using `MSE` as the `test` error metric.

```{r 9-test-mse}
ols_pred <- predict(model_lm, test)
(ols_mse <- mean((ols_pred - test$apps)^2))
```

### (c) Fit a ridge regression model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.

First, let's convert the `train` and `test` dataset to matrix type and
drop the target 'apps'.

```{r 9-train-test-matrix}
train_mat <- train %>% 
     select(-apps) %>% 
     mutate(private = private %>% as.numeric) %>% 
     as.matrix()

test_mat <- test %>% 
     select(-apps) %>% 
     mutate(private = private %>% as.numeric) %>% 
     as.matrix()
```

Use the `glmnet` package to 5-fold cross-validate ridge regression with
a range of $\lambda$ (from 0.001 to 100).

```{r 9-ridge-cv}
set.seed(2020)

model_ridge <- cv.glmnet(y = train$apps, 
                         x = train_mat, 
                         alpha = 0, 
                         lambda = 10^seq(2, -3, length = 100), 
                         standardize = TRUE, 
                         nfolds = 5)

# plot lambda values and the corresponding cv-MSE
data.frame(lambda = model_ridge$lambda, 
           cv_mse = model_ridge$cvm) %>%
     ggplot(aes(x = lambda, y = cv_mse)) + 
     geom_point() + 
     geom_line() + 
     geom_vline(xintercept = model_ridge$lambda.min, col = "deepskyblue3") +
     geom_hline(yintercept = min(model_ridge$cvm), col = "deepskyblue3") +
     scale_x_continuous(
          trans = 'log10', 
          breaks = c(0.01, 0.1, 1, 10, 100), 
          labels = c(0.01, 0.1, 1, 10, 100)
     ) + 
     scale_y_continuous(labels = scales::comma_format()) + 
     theme(legend.position = "bottom") + 
     labs(x = "Lambda", 
          y = "Cross-Validation MSE", 
          col = "Non-Zero Coefficients:", 
          title = "Ridge Regression - Lambda Selection (Using 5-Fold Cross-Validation)")
```

The selected value of $\lambda$ is 0.001.

Let's refit the model with the tuned value of $\lambda$ and apply it on
the `test` dataset.

```{r 9-tuned-lambda-test}
model_ridge_best <- glmnet(y = train$apps,
                           x = train_mat,
                           alpha = 0, 
                           lambda = 10^seq(2, -3, length = 100), 
                           standardize = TRUE)

ridge_pred <- predict(model_ridge_best, s = model_ridge$lambda.min, 
                      newx = test_mat)

(ridge_mse <- mean((ridge_pred - test$apps)^2))
```

### (d) Fit a lasso model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.

Use the `glmnet` package to 5-fold cross-validate lasso regression with
a range of $\lambda$ (from 0.00001 to 100).

```{r 9-lasso-cv}
set.seed(2020)

model_lasso <- cv.glmnet(y = train$apps, 
                         x = train_mat, 
                         alpha = 1, 
                         lambda = 10^seq(2, -5, length = 100), 
                         standardize = TRUE, 
                         nfolds = 5)

# plot lambda values and the corresponding cv-MSE
data.frame(lambda = model_lasso$lambda, 
           cv_mse = model_lasso$cvm) %>%
  ggplot(aes(x = lambda, y = cv_mse)) + 
  geom_point() + 
  geom_line() + 
  geom_vline(xintercept = model_lasso$lambda.min, col = "deepskyblue3") +
  geom_hline(yintercept = min(model_lasso$cvm), col = "deepskyblue3") +
  scale_x_continuous(
       trans = 'log10', 
       breaks = c(0.01, 0.1, 1, 10, 100), 
       labels = c(0.01, 0.1, 1, 10, 100)
       ) + 
  scale_y_continuous(labels = scales::comma_format()) + 
  theme(legend.position = "bottom") + 
  labs(x = "Lambda", 
       y = "Cross-Validation MSE", 
       col = "Non-Zero Coefficients:", 
       title = "Lasso Regression - Lambda Selection (Using 5-Fold Cross-Validation)")
```

The selected value of $\lambda$ is 0.00002257.

Let's refit the model with the tuned value of $\lambda$ and apply it on
the `test` dataset.

```{r 9-tuned-lambda-test}
model_lasso_best <- glmnet(y = train$apps,
                           x = train_mat,
                           alpha = 1, 
                           lambda = 10^seq(2, -5, length = 100), 
                           standardize = TRUE)

lasso_pred <- predict(model_lasso_best, s = model_lasso$lambda.min, 
                      newx = test_mat)

(lasso_mse <- mean((lasso_pred - test$apps)^2))
```

Lasso model coefficients:

```{r 9-lasso-coefficients}
lasso_coef <- predict(model_lasso_best, type = "coefficients", s = model_lasso$lambda.min)

round(lasso_coef, 3)
```

The lasso model has 17 predictors, which is the same number as the `ols`
model.

### (e) Fit a `PCR` (Principal Componenets Regression) model on the training set, with $M$ chosen by cross- validation. Report the test error obtained, along with the value of $M$ selected by cross-validation.

First, let's tune $M$ using cross-validation.

```{r 9-PCR-M-cv-tuning}
set.seed(500)

model_pcr <- pcr(apps ~ .,
                 data = train, 
                 scale = T, 
                 validation = "CV")

model_pcr_mse <- MSEP(model_pcr, estimate = "CV")$val %>%
     reshape2::melt() %>%
     mutate(M = 0:(nrow(.)-1)) %>%
     select(M, value) %>%
     rename(CV_MSE = value)

model_pcr_mse
```

Plot cv-MSE as a function of $M$

```{r 9-PCR-plot-M-cvMSE}
model_pcr_mse %>%
     mutate(min_CV_MSE = as.numeric(min(CV_MSE) == CV_MSE)) %>%
     ggplot(aes(x = M, y = CV_MSE)) + 
     geom_line(col = "grey55") + 
     geom_point(size = 2, aes(col = factor(min_CV_MSE))) + 
     scale_y_continuous(labels = scales::comma_format()) + 
     scale_color_manual(values = c("deepskyblue3", "green")) + 
     theme(legend.position = "none") + 
     labs(x = "M", 
          y = "Cross-Validation MSE", 
          col = "Non-Zero Coefficients:", 
          title = "PCR - M Selection (Using 10-Fold Cross-Validation)")
```

$M$ = 17 is the minimum cross-validated MSE, same as the original number
of predictors.

`test` MSE using $M$ = 17

```{r 9-PCR-test-MSE}
pcr_pred <- predict(model_pcr, test, ncomp = 17)

(pcr_mse <- mean((pcr_pred - test$apps)^2))
```

The `PCR` MSE is identical to the `OLS` MSE.

### (f) Fit a `PLS` (Partial Least Squares Regression) model on the training set, with $M$ chosen by cross- validation. Report the test error obtained, along with the value of $M$ selected by cross-validation.

Let's tune $M$ using cross-validation.

```{r 9-PLS-M-cv-tuning}
set.seed(500)

model_pls <- plsr(apps ~ .,
                 data = train, 
                 scale = T, 
                 validation = "CV")

model_pls_mse <- MSEP(model_pls, estimate = "CV")$val %>%
     reshape2::melt() %>%
     mutate(M = 0:(nrow(.)-1)) %>%
     select(M, value) %>%
     rename(CV_MSE = value)

model_pls_mse
```

Plot cv-MSE as a function of $M$

```{r 9-PLS-plot-M-cvMSE}
model_pls_mse %>%
     mutate(min_CV_MSE = as.numeric(min(CV_MSE) == CV_MSE)) %>%
     ggplot(aes(x = M, y = CV_MSE)) + 
     geom_line(col = "grey55") + 
     geom_point(size = 2, aes(col = factor(min_CV_MSE))) + 
     scale_y_continuous(labels = scales::comma_format()) + 
     scale_color_manual(values = c("deepskyblue3", "green")) + 
     theme(legend.position = "none") + 
     labs(x = "M", 
          y = "Cross-Validation MSE", 
          col = "Non-Zero Coefficients:", 
          title = "PLS - M Selection (Using 10-Fold Cross-Validation)")
```

$M$ = 16 is the minimum cross-validated MSE.

`test` MSE using $M$ = 16

```{r 9-PLS-test-MSE}
pls_pred <- predict(model_pls, test, ncomp = 16)

(pls_mse <- mean((pls_pred - test$apps)^2))
```

### (g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

Let's compare the results for each model and add the $R^2$ metric.
```{r 9-MSE-comparison-table}
# R2 = 1 - (SS_res / SS_tot)^2

SS_tot <- sum((test$apps - mean(test$apps))^2)

data.frame(method = c("OLS", "Ridge", "Lasso", "PCR", "PLS"), 
           test_MSE = c(ols_mse, ridge_mse, lasso_mse, pcr_mse, pls_mse), 
           test_R2 = c(1 - sum((test$apps - ols_pred)^2) / SS_tot,
                       1 - sum((test$apps - ridge_pred)^2) / SS_tot, 
                       1 - sum((test$apps - lasso_pred)^2) / SS_tot, 
                       1 - sum((test$apps - pcr_pred)^2) / SS_tot, 
                       1 - sum((test$apps - pls_pred)^2) / SS_tot)) %>% 
     arrange(test_MSE)
```

**Conclusions:**

1.  The `PLS` model was the best performer according to the MSE and $R^2$ metrics.  On the other side, the `PCR` was the worst performer.

2.  Taking `OLS` as the baseline model, the increase in performance by `PLS` is almost negligible (0.0046% MSE).